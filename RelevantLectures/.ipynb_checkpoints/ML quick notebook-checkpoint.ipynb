{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is meant to be a simple SKLearn tutorial/cheatsheet notebook. Written by Drace Zhan of NYCDSA for student/public use.\n",
    "Note that the mathematics behind the models will not be covered here and the examples used will be purely for artificial purposes so there won't be any preprocessing, train-test-splits, verifying assumptions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data manipulation tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#data visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading datasets used for this notebook\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading iris from dictionary to dataframe\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading boston dataset from dictionary to dataframe\n",
    "boston = datasets.load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns= boston.feature_names)\n",
    "boston_df['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression & Logistic Regression\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Model objects in Sklearn has various parameters and arguments you can pass in to the object as you create it. I highly recommend reading the documentation to understand what you can do to adjust the parameters on it. One of the more useful ones is \"n_jobs\". This argument is the amount of cores your CPU will use running the model. Setting it to -1 such as \"n_jobs = -1\" will force it to use ALL your cores. Your laptop will get quite hot during this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a Linear regression in SKlearn\n",
    "linreg_1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving the X & y labels in Python can be a bit strange. The thing to keep in mind is that you are trying to pass in the values of your X and not the column names themselves. I recommend first creating a list of your column names before passing it into X as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating X, y for Boston dataset, for this toy example, we'll be using all the columns except for Y for a toy example.\n",
    "b_feat_list = boston_df.columns[0:-1]\n",
    "X = boston_df[b_feat_list]\n",
    "y = boston_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create the model object, it will allow you to fit data into it and then will start training to your data. After the model finishes training, it will allow you to call on the various attributes of the now trained model. This will be elaborated further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitting the LinearRegression object to your data\n",
    "linreg_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the object, you'll note it now has several attributes you can call upon. Feel free to experiment with them! I will list some of the more useful ones below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting the coefficients of your LinearRegression object\n",
    "linreg_1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting the intercept\n",
    "linreg_1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the R2 score of your model\n",
    "linreg_1.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting with your model\n",
    "linreg_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#passing in another set for your model to predict\n",
    "X_test = X[0:30]\n",
    "linreg_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how you create a Linear Regression object, Logistic Regression is much the same. Some things that are useful in the Logistic Regression arguments include \"class_weight\" to handle imbalanced classes. In addition, the Logistic Regression object will automatically include regularization (ridge by default but you can set to lasso as well). You can combat this by setting the C value to a very large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar steps to \n",
    "iris_feat_list = iris_df.columns[0:-1]\n",
    "iris_X = iris_df[iris_feat_list]\n",
    "iris_y = iris_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_1.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like LinearRegression, the LogisticRegression model will have many attributes you can call upon. In addition to standard prediction, the logistic regression model will also allow you to show probabilities of your predictions as well as log probabilities as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predictions\n",
    "logit_1.predict(iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prediction probabilities\n",
    "iris_predict_data = pd.DataFrame(logit_1.predict_proba(iris_X))\n",
    "iris_predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prediction of log odds\n",
    "iris_log_predict = pd.DataFrame(logit_1.predict_log_proba(iris_X))\n",
    "iris_log_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees and Tree based models are also in Sklearn. While decision tree and random forest are often taught together, it's important to remember that most tree based models are ensembles so they are imported from different areas in sklearn.\n",
    "As tree based models have lots of parameters that are reliant on theory of trees, I won't go too indepth into them here except to highlight some common parameters that are often tuned.\n",
    "Trees can be split along according to gini or entropy as I'm using a Classifier here. It's worth evaluating which metrics gives you better performance even though both attempt to do similar things. Regressor models have other criterias as well such as \"mse\", etc. Standard things such as max_depth, min_samples_split, min_samples_leaf, etc can all be tuned as well. Note that the max_features is set to \"None\" when generally the \"standard\" practice is set it to square root of your features. Both \"sqrt\" or \"auto\" will adjust this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_tree.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examining feature importances of your tree model\n",
    "iris_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A neat way to visualize your feature importance!\n",
    "pd.Series(index = iris_feat_list, data = iris_tree.feature_importances_).sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Examining different classes in your class target\n",
    "iris_tree.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counting how many features are in your model\n",
    "iris_tree.n_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, Sklearn's Random Forest and Gradient Boosting Machine are both under ensemble models. Most of the parameters are similar to tree's model itself so I'll simply be going over some ensemble specific parameters. Note you can also use VotingClassifier from this package as a simple ensemble object if you want to play with model ensembling a bit on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things to note is that since these are ensembles of decision trees, one new parameter is \"n_estimators\". This signifies the amount of trees you desire for your ensemble. Generally a larger number such as 1000-2000 is recommended but we'll use the default 10 for this toy example. Another thing worth noting is that max_features is set to \"auto\" to designate that it will use square root of total features when considering a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_rf_1 = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_rf_1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will show you the base estimator of your ensemble, in this case, a decision tree regressor\n",
    "boston_rf_1.base_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will show you a list of ALL the estimators in your model\n",
    "boston_rf_1.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting with model\n",
    "boston_rf_1.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM is the last model I'd like to discuss in this tutorial. There's a few new metrics to take into account here. One is the loss function that you wish to optimize and the other is the learning rate. I generally don't touch these but the learning_rate can be thought of as a recipricol effect with n_estimators. So more trees, you can use a faster learning rate and vice versa but tuning these can lead to better results. Subsample is another parameter that I often tune. I've COMPLETELY forgotten the mathematical reason behind it but general good practice is to leave it at .8 or so (depends on size of your data as well!) rather than the default 1.0 for SGD purposes. Once again, for simplicity sake, I will just leave most parameters at base though here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_gbm = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_gbm.fit(iris_X, iris_y)\n",
    "iris_gbm.predict(iris_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there's another package in sklearn that's really useful which is the metrics package. For the sake of time, I don't have too much time to go into it but it's worth looking into. There's lots of different metrics there where you'll be able to evaluate such as RSME, accuracy, etc and the like. I'll show a standard convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the various metrics, simply call \"metrics.FUNCTION(true_y, predicted_y) and it'll often give you the proper score. If you have issues with this, feel free to find me. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example: metrics.accuracy_score(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to cross validate your model, in Python 3, you'll need to import it from the model_selection module in sklearn. You can also use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we will 5-fold cross validate the linear regression model from earlier as well as store the scores \n",
    "into the scores variable. You can adjust the scoring metrics as well but I'm leaving it default for now. Note the cross_validate function is imported here as well to show that you can also create an object that allows you to cross validate across multiple different scoring metrics and parameters. For simplicity sake, it won't be demo'd here but it's useful to note if you feel more comfortable with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(linreg_1, X, y, cv = 5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a holdout set using the train_test_split module from model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "clf = linreg_1.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a demo of the example as well as how to make a cross validated prediction from the training set to predict the hold out set. You can then use the metrics function mentioned previously as well on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(linreg_1, X, y, cv = 5)\n",
    "metrics.r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
